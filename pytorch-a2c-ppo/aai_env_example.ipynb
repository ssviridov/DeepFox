{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import animalai as aai\n",
    "from animalai.envs import UnityEnvironment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'env/': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls aai_resources/env/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aai_file = \"aai_resources/env/AnimalAI\" # without extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of Training Brains : 1\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(\n",
    "    file_name=aai_file, \n",
    "    worker_id = 1,\n",
    "    seed=0,\n",
    "    docker_training=False,\n",
    "    n_arenas=4,\n",
    "    play=False, \n",
    "    inference=False,\n",
    "    resolution=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Learner': <animalai.envs.brain.BrainInfo at 0x7f9375a9a198>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset(train_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_brains = {}\n",
    "for brain_name in env.external_brain_names:\n",
    "    external_brains[brain_name] = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Learner': <animalai.envs.brain.BrainParameters at 0x7f93773099e8>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_brains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aai_resources/default_configs/1-Food.yaml\n",
      "aai_resources/default_configs/exampleConfig.yaml\n",
      "aai_resources/default_configs/forcedChoice.yaml\n",
      "aai_resources/default_configs/exampleTraining.yaml\n",
      "aai_resources/default_configs/objectManipulation.yaml\n",
      "aai_resources/default_configs/2-Preferences.yaml\n",
      "aai_resources/default_configs/6-Generalization.yaml\n",
      "aai_resources/default_configs/7-InternalMemory.yaml\n",
      "aai_resources/default_configs/5-SpatialReasoning.yaml\n",
      "aai_resources/default_configs/4-Avoidance.yaml\n",
      "aai_resources/default_configs/movingFood.yaml\n",
      "aai_resources/default_configs/3-Obstacles.yaml\n",
      "aai_resources/default_configs/allObjectsRandom.yaml\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "configs = glob.glob(\"aai_resources/default_configs/*.yaml\")\n",
    "for c in configs:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import animalai as aai\n",
    "from animalai.envs import UnityEnvironment\n",
    "aai_file = \"aai_resources/env/AnimalAI\" # without extension\n",
    "\n",
    "from animalai.envs.gym.environment import AnimalAIEnv\n",
    "from animalai.envs.arena_config import ArenaConfig\n",
    "\n",
    "from a2c_ppo_acktr.aai_config_generator import SingleConfigGenerator\n",
    "from a2c_ppo_acktr.aai_wrapper import make_vec_envs_aai\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def create_env(seed, use_info=False):\n",
    "    gen_config = SingleConfigGenerator.from_file('aai_resources/default_configs/1-Food.yaml')\n",
    "    env = make_vec_envs_aai(\n",
    "        aai_file,\n",
    "        gen_config,\n",
    "        seed,\n",
    "        1,\n",
    "        None,\n",
    "        \"cpu\",\n",
    "        allow_early_resets=False,\n",
    "        use_info=use_info,\n",
    "        headless=False,\n",
    "    )\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch as th\n",
    "action = th.zeros((1,))\n",
    "action[0] = 3\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of Training Brains : 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time limit:  250\n",
      "action: tensor([3.])\n",
      "Episode #0\n",
      "curr_step: 1 reward: -0.004 vec: [ 2.98023224e-08 -3.53984833e-01  8.46737802e-01]\n",
      "curr_step: 2 reward: -0.004 vec: [ 1.19209290e-07 -2.83241272e-03  3.00361180e+00]\n",
      "curr_step: 3 reward: -0.004 vec: [ 4.76837158e-07 -1.90734863e-05  5.09659386e+00]\n",
      "curr_step: 4 reward: -0.004 vec: [4.76837158e-07 0.00000000e+00 6.94981194e+00]\n",
      "curr_step: 5 reward: -0.004 vec: [7.15255737e-07 0.00000000e+00 8.58943081e+00]\n",
      "curr_step: 6 reward: -0.004 vec: [-4.76837158e-07  0.00000000e+00  1.00400620e+01]\n",
      "curr_step: 7 reward: -0.004 vec: [-4.76837158e-07  0.00000000e+00  1.13234854e+01]\n",
      "curr_step: 8 reward: -0.004 vec: [-4.76837158e-07  0.00000000e+00  1.24589787e+01]\n",
      "curr_step: 9 reward: -0.004 vec: [-9.53674316e-07  0.00000000e+00  1.34635887e+01]\n",
      "curr_step: 10 reward: -0.004 vec: [-1.43051147e-06  0.00000000e+00  1.43524036e+01]\n",
      "curr_step: 11 reward: -0.004 vec: [-1.43051147e-06  0.00000000e+00  1.51387711e+01]\n",
      "curr_step: 12 reward: -0.004 vec: [-3.81469727e-06  0.00000000e+00  1.58344975e+01]\n",
      "curr_step: 13 reward: -0.004 vec: [-2.86102295e-06  0.00000000e+00  1.64500313e+01]\n",
      "curr_step: 14 reward: -0.004 vec: [-3.33786011e-06  0.00000000e+00  1.69946156e+01]\n",
      "curr_step: 15 reward: -0.004 vec: [-2.38418579e-06  0.00000000e+00  1.74764309e+01]\n",
      "curr_step: 16 reward: -0.004 vec: [-3.81469727e-06  0.00000000e+00  1.79027100e+01]\n",
      "curr_step: 17 reward: -0.004 vec: [-1.90734863e-06  0.00000000e+00  1.82798519e+01]\n",
      "curr_step: 18 reward: -0.004 vec: [ 0.          0.         18.61352348]\n",
      "curr_step: 19 reward: -0.004 vec: [ 0.          0.         18.90873718]\n",
      "curr_step: 20 reward: -0.004 vec: [9.53674316e-07 0.00000000e+00 1.91699219e+01]\n",
      "curr_step: 21 reward: -0.004 vec: [ 0.          0.         19.40100479]\n",
      "curr_step: 22 reward: -0.004 vec: [ 0.          0.         19.60544968]\n",
      "curr_step: 23 reward: -0.004 vec: [ 0.          0.         19.78632927]\n",
      "curr_step: 24 reward: -0.004 vec: [9.53674316e-07 0.00000000e+00 1.99463577e+01]\n",
      "curr_step: 25 reward: -0.004 vec: [-9.53674316e-07  0.00000000e+00  2.00879421e+01]\n",
      "curr_step: 26 reward: -0.004 vec: [9.53674316e-07 0.00000000e+00 2.02132111e+01]\n",
      "curr_step: 27 reward: -0.004 vec: [ 0.          0.         20.32403755]\n",
      "curr_step: 28 reward: -0.004 vec: [-9.53674316e-07  0.00000000e+00  2.04220905e+01]\n",
      "curr_step: 29 reward: -0.004 vec: [-9.53674316e-07  0.00000000e+00  2.05088387e+01]\n",
      "curr_step: 30 reward: -0.004 vec: [-9.53674316e-07  0.00000000e+00  2.05855942e+01]\n",
      "curr_step: 31 reward: -0.004 vec: [-1.90734863e-06  0.00000000e+00  2.06534996e+01]\n",
      "curr_step: 32 reward: -0.004 vec: [-2.86102295e-06  0.00000000e+00  2.07135735e+01]\n",
      "curr_step: 33 reward: -0.004 vec: [-3.81469727e-06  0.00000000e+00  2.07667274e+01]\n",
      "curr_step: 34 reward: -0.004 vec: [-3.81469727e-06  0.00000000e+00  2.08137569e+01]\n",
      "curr_step: 35 reward: -0.004 vec: [-4.76837158e-06  0.00000000e+00  2.08553619e+01]\n",
      "curr_step: 36 reward: -0.004 vec: [-3.81469727e-06  0.00000000e+00  2.08921719e+01]\n",
      "curr_step: 37 reward: -0.004 vec: [-4.76837158e-06  0.00000000e+00  2.09247398e+01]\n",
      "curr_step: 38 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 39 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 40 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 41 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 42 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 43 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 44 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 45 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 46 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 47 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 48 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 49 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 50 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 51 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 52 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 53 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 54 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 55 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 56 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 57 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 58 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 59 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 60 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 61 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 62 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 63 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 64 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 65 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 66 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 67 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 68 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 69 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 70 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 71 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 72 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 73 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 74 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 75 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 76 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 77 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 78 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 79 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 80 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 81 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 82 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 83 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 84 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 85 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 86 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 87 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 88 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 89 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 90 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 91 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 92 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 93 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 94 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 95 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 96 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 97 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 98 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 99 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 100 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 101 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 102 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 103 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 104 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 105 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 106 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 107 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 108 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 109 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 110 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 111 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 112 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 113 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 114 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 115 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 116 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 117 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 118 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 119 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 120 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 121 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 122 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 123 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 124 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 125 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 126 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 127 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 128 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 129 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 130 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 131 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 132 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 133 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 134 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 135 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 136 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 137 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 138 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 139 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 140 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 141 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 142 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 143 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 144 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 145 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 146 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 147 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 148 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 149 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 150 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 151 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 152 reward: -0.004 vec: [0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_step: 153 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 154 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 155 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 156 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 157 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 158 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 159 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 160 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 161 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 162 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 163 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 164 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 165 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 166 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 167 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 168 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 169 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 170 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 171 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 172 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 173 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 174 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 175 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 176 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 177 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 178 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 179 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 180 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 181 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 182 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 183 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 184 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 185 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 186 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 187 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 188 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 189 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 190 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 191 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 192 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 193 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 194 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 195 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 196 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 197 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 198 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 199 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 200 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 201 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 202 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 203 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 204 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 205 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 206 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 207 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 208 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 209 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 210 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 211 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 212 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 213 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 214 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 215 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 216 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 217 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 218 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 219 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 220 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 221 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 222 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 223 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 224 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 225 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 226 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 227 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 228 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 229 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 230 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 231 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 232 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 233 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 234 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 235 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 236 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 237 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 238 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 239 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 240 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 241 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 242 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 243 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 244 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 245 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 246 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 247 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 248 reward: -0.004 vec: [0. 0. 0.]\n",
      "curr_step: 249 reward: -0.004 vec: [0. 0. 0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f785fbaad77b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mepisode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.07\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/repos/baselines/baselines/common/vec_env/vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/ml/rl/DeepFox/pytorch-a2c-ppo/a2c_ppo_acktr/envs.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstacked_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_dim0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstacked_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_dim0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/ml/rl/DeepFox/pytorch-a2c-ppo/a2c_ppo_acktr/envs.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/repos/baselines/baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m#    action = int(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/ml/rl/DeepFox/pytorch-a2c-ppo/a2c_ppo_acktr/aai_wrapper.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from itertools import count\n",
    "\n",
    "env = create_env(1, use_info=True)\n",
    "try:\n",
    "    print(\"action: {}\".format(action))\n",
    "    for episode in range(4):\n",
    "        print(\"Episode #{}\".format(episode))\n",
    "        initial_observation = env.reset()\n",
    "        done = False\n",
    "        episode_rewards = 0\n",
    "        for t in count(1):\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            time.sleep(0.07)\n",
    "            print(\"curr_step: {} reward: {:.3f} vec: {}\".format(\n",
    "                t, reward.item(), info[0]['vec']\n",
    "            ))\n",
    "            episode_rewards += reward.item()\n",
    "            \n",
    "            if done: break\n",
    "\n",
    "        print(\"\\rTotal steps: {}                   \".format(t))\n",
    "        print(\"Total reward this episode: {:.3f}\".format(episode_rewards))\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
